## turn z-scores into probabilities
postexample1$prob.test <- pnorm(postexample1$yhat.test)
## average over the posterior samples
postexample1$prob.test.mean <- apply(postexample1$prob.test, 2, mean)
probcensbelow <- postexample1$prob.test.mean
tempreslist$probitBART_brier_below <- mean( (postexample1$prob.test.mean  -  Y_bin_testing_below )^2)
postexample1 <- pbart(X_matrix_train,
Y_bin_training_above,
X_matrix_test,
nskip=num_burnin,
ndpost=num_iter)
## Not run:
## turn z-scores into probabilities
postexample1$prob.test <- pnorm(postexample1$yhat.test)
## average over the posterior samples
postexample1$prob.test.mean <- apply(postexample1$prob.test, 2, mean)
probcensabove <- postexample1$prob.test.mean
tempreslist$probitBART_brier_above <- mean( (postexample1$prob.test.mean  -  Y_bin_testing_above )^2)
postexample1 <- pbart(X_matrix_train,
Y_bin_training_both,
X_matrix_test,
nskip=num_burnin,
ndpost=num_iter)
## Not run:
## turn z-scores into probabilities
postexample1$prob.test <- pnorm(postexample1$yhat.test)
## average over the posterior samples
postexample1$prob.test.mean <- apply(postexample1$prob.test, 2, mean)
tempreslist$probitBART_brier <- mean( (postexample1$prob.test.mean  -  Y_bin_testing_both )^2)
predprobitBART <- prediction(postexample1$prob.test.mean,Y_bin_testing_both)
probitBART_perf <- performance(predprobitBART, measure = "auc")
tempreslist$probitBART_AUC <- probitBART_perf@y.values
postexample1_cont <- wbart(X_matrix_train,
y_train_trans,
X_matrix_test,
nskip=num_burnin,
ndpost=num_iter)
cont_bart_preds <- postexample1_cont$yhat.test.mean
tempreslist$probitBART_MSE_uncond_obs <- mean( ( cont_bart_preds  -  y_test_trans )^2)
tempreslist$probitBART_MSE_latent <- mean( ( cont_bart_preds  -  ylatent_test )^2)
tempreslist$probitBART_MSE_mu <- mean( ( cont_bart_preds  -  truelatentmu_test )^2)
bartdrawstest <- matrix(NA,
nrow = num_iter,
ncol = ntest)
for (j in 1:ntest){
bartdrawstest[,j] <- postexample1_cont$yhat.test[,j] + rnorm(num_iter, mean = 0, sd = postexample1_cont$sigma)
}
bart_latent_PI_mat <- apply(bartdrawstest,2,quantile, prob = c(0.025,0.975))
# bart_obs_PI_mat <- apply(bartmcmc_example$test.y_withcensoring,1,quantile, prob = c(0.025,0.975))
# not sure whether to censor these draws or not
bart_obs_PI_mat <- bart_latent_PI_mat
tempreslist$probitBART_len_latent <- mean(bart_latent_PI_mat[2,] - bart_latent_PI_mat[1,])
bart_latent_PIs_cov_inds <- (bart_latent_PI_mat[2,] >= ylatent_test) & (bart_latent_PI_mat[1,] <= ylatent_test)
tempreslist$probitBART_cov_latent <- mean(bart_latent_PIs_cov_inds)
tempreslist$probitBART_len <- mean(bart_obs_PI_mat[2,] - bart_obs_PI_mat[1,])
bart_obs_PIs_cov_inds <- (bart_obs_PI_mat[2,] >= y_test_trans) & (bart_obs_PI_mat[1,] <= y_test_trans)
tempreslist$probitBART_cov <- mean(bart_obs_PIs_cov_inds)
rm(bart_obs_PI_mat, bart_latent_PI_mat)
gc()
gc()
y_pred <- cont_bart_preds
if(below_cens == - Inf){
if(above_cens == Inf){
condexptest <- (y_pred )
}else{ # above_cens !=Inf
condexptest <-
(y_pred )*(1- probcensabove ) +
above_cens*probcensabove
}
}else{ # below_cens != - Inf
if(above_cens == Inf){
condexptest <- below_cens*probcensbelow +
(y_pred )*(1 - probcensbelow)
}else{ # above_cens !=Inf
condexptest <- below_cens*probcensbelow +
(y_pred )*(1- probcensabove - probcensbelow) +
above_cens*probcensabove
}
}
tempreslist$probitBART_MSE <- mean( ( condexptest  -  y_test_trans )^2)
################### Soft BART #######################################################
opts <- Opts(num_burn = num_burnin, num_save = num_iter)
tempdftrain <- data.frame(X = X_matrix_train,
Y = factor(Y_bin_training_below, levels = c(0,1)) )
tempdftest <- data.frame(X = X_matrix_test, Y = factor(rep(0,nrow(X_matrix_test)), levels = c(0,1))
)
postexample1 <- softbart_probit(Y ~ .,
tempdftrain,
tempdftest,
opts = opts)
## Not run:
## turn z-scores into probabilities
# postexample1$prob.test <- pnorm(postexample1$yhat.test)
## average over the posterior samples
# postexample1$prob.test.mean <- apply(postexample1$prob.test, 2, mean)
probcensbelow <- postexample1$p_test_mean
tempreslist$soft_probitBART_brier_below <- mean( (postexample1$p_test_mean  -  Y_bin_testing_below )^2)
tempdftrain <- data.frame(X = X_matrix_train,
Y = factor(Y_bin_training_above, levels = c(0,1)) )
tempdftest <- data.frame(X = X_matrix_test, Y = factor(rep(0,nrow(X_matrix_test)), levels = c(0,1)))
postexample1 <- softbart_probit(Y ~ .,
tempdftrain,
tempdftest,
opts = opts)
# postexample1 <- pbart(X_matrix_train,
#                       Y_bin_training_above,
#                       X_matrix_test,
#                       nskip=num_burnin,
#                       ndpost=num_iter)
## Not run:
## turn z-scores into probabilities
# postexample1$prob.test <- pnorm(postexample1$yhat.test)
## average over the posterior samples
# postexample1$prob.test.mean <- apply(postexample1$prob.test, 2, mean)
probcensabove <- postexample1$p_test_mean
tempreslist$soft_probitBART_brier_above <- mean( (postexample1$p_test_mean  -  Y_bin_testing_above )^2)
tempdftrain <- data.frame(X = X_matrix_train,
Y = factor(Y_bin_training_both, levels = c(0,1)) )
tempdftest <- data.frame(X = X_matrix_test, Y = factor(rep(0,nrow(X_matrix_test)), levels = c(0,1)))
postexample1 <- softbart_probit(Y ~ .,
tempdftrain,
tempdftest,
opts = opts)
# postexample1 <- pbart(X_matrix_train,
#                       Y_bin_training_both,
#                       X_matrix_test,
#                       nskip=num_burnin,
#                       ndpost=num_iter)
## Not run:
## turn z-scores into probabilities
# postexample1$prob.test <- pnorm(postexample1$yhat.test)
## average over the posterior samples
# postexample1$prob.test.mean <- apply(postexample1$prob.test, 2, mean)
tempreslist$soft_probitBART_brier <- mean( (postexample1$p_test_mean  -  Y_bin_testing_both )^2)
predprobitBART <- prediction(postexample1$p_test_mean,Y_bin_testing_both)
probitBART_perf <- performance(predprobitBART, measure = "auc")
tempreslist$soft_probitBART_AUC <- probitBART_perf@y.values
tempdftrain <- data.frame(X = X_matrix_train,
Y = y_train_trans )
tempdftest <- data.frame(X = X_matrix_test, Y = rep(0,nrow(X_matrix_test)))
postexample1_cont <- softbart_regression(Y ~ .,
tempdftrain,
tempdftest,
opts = opts)
# postexample1_cont <- wbart(X_matrix_train,
#                            y_train_trans,
#                            X_matrix_test,
#                            nskip=num_burnin,
#                            ndpost=num_iter)
cont_bart_preds <- postexample1_cont$mu_test_mean
tempreslist$soft_probitBART_MSE_uncond_obs <- mean( ( cont_bart_preds  -  y_test_trans )^2)
tempreslist$soft_probitBART_MSE_latent <- mean( ( cont_bart_preds  -  ylatent_test )^2)
tempreslist$soft_probitBART_MSE_mu <- mean( ( cont_bart_preds  -  truelatentmu_test )^2)
# postexample1_cont$mu_test
# postexample1_cont$sigma
bartdrawstest <- matrix(NA,
nrow = num_iter,
ncol = ntest)
for (j in 1:ntest){
bartdrawstest[,j] <- postexample1_cont$mu_test[,j] + rnorm(num_iter, mean = 0, sd = postexample1_cont$sigma)
}
bart_latent_PI_mat <- apply(bartdrawstest,2,quantile, prob = c(0.025,0.975))
# bart_obs_PI_mat <- apply(bartmcmc_example$test.y_withcensoring,1,quantile, prob = c(0.025,0.975))
# not sure whether to censor these draws or not
bart_obs_PI_mat <- bart_latent_PI_mat
tempreslist$soft_probitBART_len_latent <- mean(bart_latent_PI_mat[2,] - bart_latent_PI_mat[1,])
bart_latent_PIs_cov_inds <- (bart_latent_PI_mat[2,] >= ylatent_test) & (bart_latent_PI_mat[1,] <= ylatent_test)
tempreslist$soft_probitBART_cov_latent <- mean(bart_latent_PIs_cov_inds)
tempreslist$soft_probitBART_len <- mean(bart_obs_PI_mat[2,] - bart_obs_PI_mat[1,])
bart_obs_PIs_cov_inds <- (bart_obs_PI_mat[2,] >= y_test_trans) & (bart_obs_PI_mat[1,] <= y_test_trans)
tempreslist$soft_probitBART_cov <- mean(bart_obs_PIs_cov_inds)
y_pred <- cont_bart_preds
if(below_cens == - Inf){
if(above_cens == Inf){
condexptest <- (y_pred )
}else{ # above_cens !=Inf
condexptest <-
(y_pred )*(1- probcensabove ) +
above_cens*probcensabove
}
}else{ # below_cens != - Inf
if(above_cens == Inf){
condexptest <- below_cens*probcensbelow +
(y_pred )*(1 - probcensbelow)
}else{ # above_cens !=Inf
condexptest <- below_cens*probcensbelow +
(y_pred )*(1- probcensabove - probcensbelow) +
above_cens*probcensabove
}
}
tempreslist$soft_probitBART_MSE <- mean( ( condexptest  -  y_test_trans )^2)
rm(postexample1_cont,bart_obs_PI_mat, bart_latent_PI_mat)
gc()
gc()
############Logit BART ############
# postexample2 <- lbart(X_matrix_train,
#                       Y_bin_training_below,
#                       X_matrix_test,
#                       nskip=5000,
#                       ndpost=10000)
# ## Not run:
# ## turn z-scores into probabilities
# postexample2$prob.test <- plogis(postexample2$yhat.test)
# ## average over the posterior samples
# postexample2$prob.test.mean <- apply(postexample2$prob.test, 2, mean)
#
#
# logitBART_brier_below[i] <- mean( (postexample2$prob.test.mean  -  Y_bin_testing_below)^2)
#
#
# postexample2 <- lbart(X_matrix_train,
#                       Y_bin_training_above,
#                       X_matrix_test,
#                       nskip=5000,
#                       ndpost=10000)
# ## Not run:
# ## turn z-scores into probabilities
# postexample2$prob.test <- plogis(postexample2$yhat.test)
# ## average over the posterior samples
# postexample2$prob.test.mean <- apply(postexample2$prob.test, 2, mean)
#
#
# logitBART_brier_above[i] <- mean( (postexample2$prob.test.mean  -  Y_bin_testing_above )^2)
#
#
# postexample2 <- lbart(X_matrix_train,
#                       Y_bin_training_both,
#                       X_matrix_test,
#                       nskip=5000,
#                       ndpost=10000)
# ## Not run:
# ## turn z-scores into probabilities
# postexample2$prob.test <- plogis(postexample2$yhat.test)
# ## average over the posterior samples
# postexample2$prob.test.mean <- apply(postexample2$prob.test, 2, mean)
#
#
# logitBART_brier[i] <- mean( (postexample2$prob.test.mean  -  Y_bin_testing_both )^2)
#
#
# predlogitBART <- prediction(postexample2$prob.test.mean,Y_bin_testing_both)
#
# logitBART_perf <- performance(predlogitBART, measure = "auc")
#
# logitBART_AUC[i] <- logitBART_perf@y.values
################# Logistic regression ###################################################
#logistic regression
#data.frame(Y_all_training , X_matrix_train)
# Fit the model
model_logreg <- glm( Y_bin_training_both  ~., data = data.frame(Y_bin_training_both ,
X_matrix_train), family = binomial)
# Make predictions
probabilities_logreg <- model_logreg %>% predict(data.frame(X_matrix_test), type = "response")
tempreslist$logreg_brier <- mean( (probabilities_logreg  -  Y_bin_testing_both )^2)
predlogreg <- prediction(probabilities_logreg,Y_bin_testing_both)
logreg_perf <- performance(predlogreg, measure = "auc")
tempreslist$logreg_AUC <- logreg_perf@y.values
################## Probit and LM ##################################################
#probit regression
model_probit <- glm( Y_bin_training_below  ~., data = data.frame(Y_bin_training_below ,
X_matrix_train), family = binomial(link = "probit"))
# Make predictions
probabilities_probit <- model_probit %>% predict(data.frame(X_matrix_test), type = "response")
probcensbelow <- probabilities_probit
tempreslist$probit_brier_below <- mean( (probabilities_probit  -  Y_bin_testing_below )^2)
model_probit <- glm( Y_bin_training_above  ~., data = data.frame(Y_bin_training_above ,
X_matrix_train), family = binomial(link = "probit"))
# Make predictions
probabilities_probit <- model_probit %>% predict(data.frame(X_matrix_test), type = "response")
probcensabove <- probabilities_probit
tempreslist$probit_brier_above <- mean( (probabilities_probit  -  Y_bin_testing_above )^2)
#data.frame(Y_all_training , X_matrix_train)
# Fit the model
model_probit <- glm( Y_bin_training_both  ~., data = data.frame(Y_bin_training_both ,
X_matrix_train), family = binomial(link = "probit"))
# Make predictions
probabilities_probit <- model_probit %>% predict(data.frame(X_matrix_test), type = "response")
tempreslist$probit_brier <- mean( (probabilities_probit  -  Y_bin_testing_both )^2)
predprobit <- prediction(probabilities_probit,Y_bin_testing_both)
probit_perf <- performance(predprobit, measure = "auc")
tempreslist$probit_AUC <- probit_perf@y.values
# linear model for continuous outcomes
model_lm <- lm( y_train_trans  ~., data = data.frame(y_train_trans ,
X_matrix_train))
y_pred <- predict(model_lm, data = data.frame(X_matrix_test))
tempreslist$probit_MSE_uncond_obs <- mean( ( y_pred  -  y_test_trans )^2)
tempreslist$probit_MSE_latent <- mean( ( y_pred  -  ylatent_test )^2)
tempreslist$probit_MSE_mu <- mean( ( y_pred  -  truelatentmu_test )^2)
# hurdle model predictions
if(below_cens == - Inf){
if(above_cens == Inf){
condexptest <- (y_pred )
}else{ # above_cens !=Inf
condexptest <-
(y_pred )*(1- probcensabove ) +
above_cens*probcensabove
}
}else{ # below_cens != - Inf
if(above_cens == Inf){
condexptest <- below_cens*probcensbelow +
(y_pred )*(1 - probcensbelow)
}else{ # above_cens !=Inf
condexptest <- below_cens*probcensbelow +
(y_pred )*(1- probcensabove - probcensbelow) +
above_cens*probcensabove
}
}
tempreslist$probit_MSE <- mean( ( condexptest  -  y_test_trans )^2)
#################### RF ###################################################
rf_fit <-  ranger(Y_bin_training_below  ~., data = data.frame(Y_bin_training_below , X_matrix_train),
probability = TRUE,
num.trees = 10000,
num.threads = 1)
rf_probs <- predict(rf_fit, data = data.frame(X_matrix_test),
num.threads = 1)
rf_probs <- rf_probs$predictions[,which(colnames(rf_fit$predictions)==1)]
probcensbelow <- rf_probs
tempreslist$rf_brier_below <- mean( (rf_probs  -  Y_bin_testing_below )^2)
rf_fit <-  ranger(Y_bin_training_above  ~., data = data.frame(Y_bin_training_above , X_matrix_train),
probability = TRUE,
num.trees = 10000,
num.threads = 1)
rf_probs <- predict(rf_fit, data = data.frame(X_matrix_test),
num.threads = 1)
rf_probs <- rf_probs$predictions[,which(colnames(rf_fit$predictions)==1)]
probcensabove <- rf_probs
tempreslist$rf_brier_above <- mean( (rf_probs  -  Y_bin_testing_above )^2)
rf_fit <-  ranger(Y_bin_training_both  ~., data = data.frame(Y_bin_training_both , X_matrix_train),
probability = TRUE,
num.trees = 10000,
num.threads = 1)
rf_probs <- predict(rf_fit, data = data.frame(X_matrix_test),
num.threads = 1)
rf_probs <- rf_probs$predictions[,which(colnames(rf_fit$predictions)==1)]
tempreslist$rf_brier <- mean( (rf_probs  -  Y_bin_testing_both )^2)
predrf <- prediction(rf_probs , Y_bin_testing_both)
rf_perf <- performance(predrf, measure = "auc")
tempreslist$rf_AUC <- rf_perf@y.values
rf_fit_cont <-  ranger(y_train_trans  ~., data = data.frame(y_train_trans , X_matrix_train),
# probability = TRUE,
num.trees = 10000,
num.threads = 1)
rf_contpreds <- predict(rf_fit_cont, data = data.frame(X_matrix_test),
num.threads = 1)$predictions
tempreslist$rf_MSE_uncond_obs <- mean( (rf_contpreds  -  y_test_trans )^2)
tempreslist$rf_MSE_latent <- mean( ( rf_contpreds  -  ylatent_test )^2)
tempreslist$rf_MSE_mu <- mean( ( rf_contpreds  -  truelatentmu_test )^2)
y_pred <- rf_contpreds
if(below_cens == - Inf){
if(above_cens == Inf){
condexptest <- (y_pred )
}else{ # above_cens !=Inf
condexptest <-
(y_pred )*(1- probcensabove ) +
above_cens*probcensabove
}
}else{ # below_cens != - Inf
if(above_cens == Inf){
condexptest <- below_cens*probcensbelow +
(y_pred )*(1 - probcensbelow)
}else{ # above_cens !=Inf
condexptest <- below_cens*probcensbelow +
(y_pred )*(1- probcensabove - probcensbelow) +
above_cens*probcensabove
}
}
tempreslist$rf_MSE <- mean( ( condexptest  -  y_test_trans )^2)
################### Grabit ####################################################
yl <- below_lim
yu <- above_lim
num_folds <- 5
flds <- createFolds(y_train_trans, k = num_folds, list = TRUE, returnTrain = FALSE)
sigma_vals <- c(0.01,0.1,1,10,100)
#number of trees
M_vals <- c(10,100,1000)
lrate_vals <- c(0.1,0.01,0.001)
depth_vals <- c(3,5,10)
allcvpars <- expand.grid(sigma_vals,M_vals,lrate_vals,depth_vals)
cv_errors <- matrix(NA,
nrow = num_folds,
ncol = nrow(allcvpars))
for(k_ind in 1:5){
Xtrain_k <- X_matrix_train[-flds[[k_ind]],]
ytrain_k <- y_train_trans[-flds[[k_ind]]]
Xtest_k <- X_matrix_train[flds[[k_ind]],]
ytest_k <- y_train_trans[flds[[k_ind]]]
if(any(ytrain_k < yl)){
print("ytrain_k =")
print(ytrain_k)
print("yu =")
print(yu)
stop("outcome below limit")
}
if(any(ytrain_k > yu)){
print("ytrain_k =")
print(ytrain_k)
print("yu =")
print(yu)
stop("outcome above limit")
}
dtrain <- gpb.Dataset(data = Xtrain_k, label = ytrain_k)
for(sig_ind in 1:nrow(allcvpars)){
bst <- gpb.train(data = dtrain, objective = "tobit",
verbose = 0, yl = yl, yu = yu,
sigma = allcvpars[sig_ind,1],
params = list(num_iterations = allcvpars[sig_ind,2],
learning_rate = allcvpars[sig_ind,3],
max_depth = allcvpars[sig_ind,4]))
y_pred <- predict(bst, data = Xtest_k)
# mean square error (approx. 1.0 for n=10'000)
# cv_errors[k_ind, sig_ind] <- mean((y_pred - ytest_k)^2)
sigtemp <- allcvpars[sig_ind,1]
# try cross validatiaon with log likelihood
cv_errors[k_ind, sig_ind] <- sum(
- ifelse(( ytest_k <= yl),
log(pnorm( (yl - y_pred)/sigtemp  )),
0  ) +
ifelse(( ytest_k > yl)&( ytest_k < yu),
( ((y_pred - ytest_k)^2 /(2* sigtemp^2)  ) + log(sigtemp) + 0.5*log(2*pi)  ),
0 )
-  ifelse( ( ytest_k >= yu),
log(1 - pnorm( (yu - y_pred)/sigtemp ) ),
0) )
print("fold number = ")
print(k_ind)
print("param iter = ")
print(sig_ind)
}
}
cvmeans <- colMeans(cv_errors)
sigma_opt <- allcvpars[which.min(cvmeans),1]
M_opt <- allcvpars[which.min(cvmeans),2]
lrate_opt <- allcvpars[which.min(cvmeans),3]
depth_opt <- allcvpars[which.min(cvmeans),4]
if(any(y_train_trans < yl)){
print("y_train_trans =")
print(y_train_trans)
print("yu =")
print(yu)
stop("outcome below limit")
}
if(any(y_train_trans > yu)){
print("y_train_trans =")
print(y_train_trans)
print("yu =")
print(yu)
stop("outcome above limit")
}
# train model and make predictions
dtrain <- gpb.Dataset(data = X_matrix_train, label = y_train_trans)
bst <- gpb.train(data = dtrain, objective = "tobit",
verbose = 0, yl = yl, yu = yu,
sigma = sigma_opt,
params = list(num_iterations = M_opt,
learning_rate = lrate_opt,
max_depth = depth_opt))
# train model and make predictions
# dtrain <- gpb.Dataset(data = X_matrix_train, label = y_train_trans)
# bst <- gpb.train(data = dtrain, nrounds = 100, objective = "tobit",
#                  verbose = 0, yl = yl, yu = yu,
#                  sigma = sigma_opt)
y_pred <- predict(bst, data = X_matrix_test)
# mean square error (approx. 1.0 for n=10'000)
# conditional expectation
below_cens <- yl
above_cens <- yu
probcensbelow <- pnorm(below_cens, mean = y_pred, sd = sigma_opt)
probcensabove <- 1 - pnorm(above_cens, mean = y_pred, sd = sigma_opt)
if(below_cens == - Inf){
if(above_cens == Inf){
condexptest <- (y_pred )
probcensoverall <- 0
}else{ # above_cens !=Inf
condexptest <-
(y_pred )*(1- probcensabove ) +
sigma_opt*(  -dnorm(above_cens, mean = y_pred, sd = sigma_opt) ) +
above_cens*probcensabove
probcensoverall <- probcensabove
}
}else{ # below_cens != - Inf
if(above_cens == Inf){
condexptest <- below_cens*probcensbelow +
(y_pred )*(1 - probcensbelow) +
sigma_opt*( dnorm(below_cens, mean = y_pred, sd = sigma_opt)  )
probcensoverall <- probcensbelow
}else{ # above_cens !=Inf
condexptest <- below_cens*probcensbelow +
(y_pred )*(1- probcensabove - probcensbelow) +
sigma_opt*( dnorm(below_cens, mean = y_pred, sd = sigma_opt) -
dnorm(above_cens, mean = y_pred, sd = sigma_opt) ) +
above_cens*probcensabove
probcensoverall <- probcensbelow + probcensabove
}
}
########## evaluate Grabit #########
#
#   grabit_probs <- predict(grabit_fit, data = data.frame(X_matrix_test))
#
#   grabit_probs <- grabit_probs$predictions[,which(colnames(grabit_fit$predictions)==1)]
tempreslist$grabit_brier_below <- mean( (probcensbelow  -  Y_bin_testing_below )^2)
tempreslist$grabit_brier_above <- mean( (probcensabove  -  Y_bin_testing_above )^2)
tempreslist$grabit_brier <- mean( (probcensoverall  -  Y_bin_testing_both )^2)
predgrabit <- prediction(probcensoverall , Y_bin_testing_both)
grabit_perf <- performance(predgrabit, measure = "auc")
tempreslist$grabit_AUC <- grabit_perf@y.values
tempreslist$grabit_MSE <- mean( (condexptest  -  y_test_trans )^2)
tempreslist$grabit_MSE_latent <- mean( ( y_pred  -  ylatent_test )^2)
tempreslist$grabit_MSE_mu <- mean( ( y_pred  -  truelatentmu_test )^2)
tempreslist$grabit_MSE_uncond_obs <- mean( ( y_pred  -  y_test_trans )^2)
res_list[[i]] <- tempreslist
# return(tempreslist)
} # end of parallelized for loop function
