% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/softtbart1.R
\name{softtbart1}
\alias{softtbart1}
\title{Type I Tobit Soft Bayesian Additive Regression Trees with sparsity inducing hyperprior implemented using MCMC}
\usage{
softtbart1(
  x.train,
  x.test,
  y,
  n.iter = 1000,
  n.burnin = 100,
  below_cens = 0,
  above_cens = Inf,
  n.trees = 50L,
  SB_group = NULL,
  SB_alpha = 1,
  SB_beta = 2,
  SB_gamma = 0.95,
  SB_k = 2,
  SB_sigma_hat = NULL,
  SB_shape = 1,
  SB_width = 0.1,
  SB_alpha_scale = NULL,
  SB_alpha_shape_1 = 0.5,
  SB_alpha_shape_2 = 1,
  SB_tau_rate = 10,
  SB_num_tree_prob = NULL,
  SB_temperature = 1,
  SB_weights = NULL,
  SB_normalize_Y = TRUE,
  print.opt = 100,
  fast = TRUE
)
}
\arguments{
\item{x.train}{The training covariate data for all training observations. Number of rows equal to the number of observations. Number of columns equal to the number of covariates.}

\item{x.test}{The test covariate data for all test observations. Number of rows equal to the number of observations. Number of columns equal to the number of covariates.}

\item{y}{The training data vector of outcomes. A continuous, censored outcome variable.}

\item{n.iter}{Number of iterations excluding burnin.}

\item{n.burnin}{Number of burnin iterations.}

\item{below_cens}{Number at or below which observations are censored.}

\item{above_cens}{Number at or above which observations are censored.}

\item{n.trees}{A positive integer giving the number of trees used in the sum-of-trees formulation.}

\item{print.opt}{Print every print.opt number of Gibbs samples.}

\item{fast}{If equal to TRUE, then implements faster truncated normal draws and approximates normal pdf.}
}
\value{
The following objects are returned:
\item{Z.matcens}{Matrix of draws of latent (censored) outcomes for censored observations. Number of rows equals number of censored training observations. Number of columns equals n.iter . Rows are ordered in order of censored observations in the training data.}
\item{Z.matcensbelow}{Matrix of draws of latent (censored) outcomes for observations censored from below. Number of rows equals number of training observations censored from below. Number of columns equals n.iter . Rows are ordered in order of censored observations in the training data. }
\item{Z.matcensabove}{Matrix of draws of latent (censored) outcomes for observations censored from above. Number of rows equals number of training observations censored from above. Number of columns equals n.iter . Rows are ordered in order of censored observations in the training data. }
\item{mu}{Matrix of draws of the sum of terminal nodes, i.e. f(x_i), for all training observations. Number of rows equals number of training observations. Number of columns equals n.iter .}
\item{mucens}{Matrix of draws of the sum of terminal nodes, i.e. f(x_i), for all censored training observations. Number of rows equals number of censored training observations. Number of columns equals n.iter .}
\item{muuncens}{Matrix of draws of the sum of terminal nodes, i.e. f(x_i), for all uncensored training observations. Number of rows equals number of uncensored training observations. Number of columns equals n.iter .}
\item{mucensbelow}{Matrix of draws of the sum of terminal nodes, i.e. f(x_i), for all training observations censored from below. Number of rows equals number of training observations censored from below. Number of columns equals n.iter .}
\item{mucensabove}{Matrix of draws of the sum of terminal nodes, i.e. f(x_i), for all training observations censored from above Number of rows equals number of training observations censored from above Number of columns equals n.iter .}
\item{ystar}{Matrix of training sample draws of the outcome assuming uncensored (can take values below below_cens and above above_cens. Number of rows equals number of training observations. Number of columns equals n.iter .}
\item{ystarcens}{Matrix of censored training sample draws of the outcome assuming uncensored (can take values below below_cens and above above_cens. Number of rows equals number of censored training observations. Number of columns equals n.iter .}
\item{ystaruncens}{Matrix of uncensored training sample draws of the outcome assuming uncensored (can take values below below_cens and above above_cens. Number of rows equals number of uncensored training observations. Number of columns equals n.iter .}
\item{ystarcensbelow}{Matrix of censored from below training sample draws of the outcome assuming uncensored (can take values below below_cens and above above_cens. Number of rows equals number of training observations censored from below. Number of columns equals n.iter .}
\item{ystarcensabove}{Matrix of censored from above training sample draws of the outcome assuming uncensored (can take values below below_cens and above above_cens. Number of rows equals number of training observations censored from above. Number of columns equals n.iter .}
\item{test.mu}{Matrix of draws of the sum of terminal nodes, i.e. f(x_i), for all test observations. Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{test.y_nocensoring}{Matrix of test sample draws of the outcome assuming uncensored. Can take values below below_cens and above above_cens. Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{test.y_withcensoring}{Matrix of test sample draws of the outcome assuming censored. Cannot take values below below_cens and above above_cens. Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{test.probcensbelow}{Matrix of draws of probabilities of test sample observations being censored from below. Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{test.probcensabove}{Matrix of draws of probabilities of test sample observations being censored from above. Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{sigma}{Vector of draws of the standard deviation of the error term. Number of elements equals n.iter .}
}
\description{
Type I Tobit Soft Bayesian Additive Regression Trees with sparsity inducing hyperprior implemented using MCMC
}
\examples{

#example taken from https://stats.idre.ucla.edu/r/dae/tobit-models/

dat <- read.csv("https://stats.idre.ucla.edu/stat/data/tobit.csv")

train_inds <- sample(1:200,190)
test_inds <- (1:200)[-train_inds]

ytrain <- dat$apt[train_inds]
ytest <- dat$apt[test_inds]

xtrain <- cbind(dat$read, dat$math)[train_inds,]
xtest <- cbind(dat$read, dat$math)[test_inds,]

tobart_res <- tbart1(xtrain,xtest,ytrain,
                    below_cens = -Inf,
                    above_cens = 800,
                    n.iter = 400,
                    n.burnin = 100)

}
