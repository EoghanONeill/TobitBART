% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tbart2np.R
\name{tbart2np}
\alias{tbart2np}
\title{Nonparametric Type II Tobit Bayesian Additive Regression Trees implemented using MCMC}
\usage{
tbart2np(
  x.train,
  x.test,
  w.train,
  w.test,
  y,
  n.iter = 1000,
  n.burnin = 100,
  censored_value = NA,
  gamma0 = 0,
  G0 = 10,
  nzero = 6,
  S0 = 12,
  sigest = NA,
  n.trees_outcome = 50L,
  n.trees_censoring = 50L,
  n.burn = 0L,
  n.samples = 1L,
  n.thin = 1L,
  n.chains = 1L,
  n.threads = guessNumCores(),
  printEvery = 100L,
  printCutoffs = 0L,
  rngKind = "default",
  rngNormalKind = "default",
  rngSeed = NA_integer_,
  updateState = TRUE,
  tree_power_z = 2,
  tree_power_y = 2,
  tree_base_z = 0.95,
  tree_base_y = 0.95,
  node.prior = dbarts:::normal,
  resid.prior = dbarts:::chisq,
  proposal.probs = c(birth_death = 0.5, swap = 0.1, change = 0.4, birth = 0.5),
  sigmadbarts = NA_real_,
  print.opt = 100,
  accelerate = FALSE,
  cov_prior = "VH",
  tau = 0.5,
  M_mat = 2 * diag(2),
  alpha_prior = "vh",
  c1 = 2,
  c2 = 2,
  alpha_gridsize = 100L,
  selection_test = 1,
  init.many.clust = TRUE,
  nu0 = 3,
  quantsig = 0.95,
  mixstep = TRUE,
  simultaneous_covmat = TRUE,
  sparse = FALSE,
  alpha_a_y = 0.5,
  alpha_b_y = 1,
  alpha_a_z = 0.5,
  alpha_b_z = 1,
  alpha_split_prior = TRUE
)
}
\arguments{
\item{x.train}{The outcome model training covariate data for all training observations. Number of rows equal to the number of observations. Number of columns equal to the number of covariates.}

\item{x.test}{The outcome model test covariate data for all test observations. Number of rows equal to the number of observations. Number of columns equal to the number of covariates.}

\item{w.train}{The censoring model training covariate data for all training observations. Number of rows equal to the number of observations. Number of columns equal to the number of covariates.}

\item{w.test}{The censoring model test covariate data for all test observations. Number of rows equal to the number of observations. Number of columns equal to the number of covariates.}

\item{y}{The training data vector of outcomes. A continuous, censored outcome variable. Censored observations must be included with values equal to censored_value}

\item{n.iter}{Number of iterations excluding burnin.}

\item{n.burnin}{Number of burnin iterations.}

\item{censored_value}{The value taken by censored observations}

\item{gamma0}{The mean of the normal prior on the covariance of the errors in the censoring and outcome models.}

\item{G0}{The variance of the normal prior on the covariance of the errors in the censoring and outcome models.}

\item{nzero}{A prior parameter which when divided by 2 gives the mean of the normal prior on phi, where phi*gamma is the variance of the errors of the outcome model.}

\item{S0}{A prior parameter which when divided by 2 gives the variance of the normal prior on phi, where phi*gamma is the variance of the errors of the outcome model.}

\item{sigest}{If variance of the error term is the}

\item{n.trees_outcome}{(dbarts control option) A positive integer giving the number of trees used in the outcome model sum-of-trees formulation.}

\item{n.trees_censoring}{(dbarts control option) A positive integer giving the number of trees used in the censoring model sum-of-trees formulation.}

\item{n.chains}{(dbarts control option) A positive integer detailing the number of independent chains for the dbarts sampler to use (more than one chain is unlikely to improve speed because only one sample for each call to dbarts).}

\item{n.threads}{(dbarts control option) A positive integer controlling how many threads will be used for various internal calculations, as well as the number of chains. Internal calculations are highly optimized so that single-threaded performance tends to be superior unless the number of observations is very large (>10k), so that it is often not necessary to have the number of threads exceed the number of chains.}

\item{printEvery}{(dbarts control option)If verbose is TRUE, every printEvery potential samples (after thinning) will issue a verbal statement. Must be a positive integer.}

\item{printCutoffs}{(dbarts control option) A non-negative integer specifying how many of the decision rules for a variable are printed in verbose mode}

\item{rngKind}{(dbarts control option) Random number generator kind, as used in set.seed. For type "default", the built-in generator will be used if possible. Otherwise, will attempt to match the built-in generator’s type. Success depends on the number of threads.}

\item{rngNormalKind}{(dbarts control option) Random number generator normal kind, as used in set.seed. For type "default", the built-in generator will be used if possible. Otherwise, will attempt to match the built-in generator’s type. Success depends on the number of threads and the rngKind}

\item{rngSeed}{(dbarts control option) Random number generator seed, as used in set.seed. If the sampler is running single-threaded or has one chain, the behavior will be as any other sequential algorithm. If the sampler is multithreaded, the seed will be used to create an additional pRNG object, which in turn will be used sequentially seed the threadspecific pRNGs. If equal to NA, the clock will be used to seed pRNGs when applicable.}

\item{updateState}{(dbarts control option) Logical setting the default behavior for many sampler methods with regards to the immediate updating of the cached state of the object. A current, cached state is only useful when saving/loading the sampler.}

\item{tree_power_z}{Tree prior parameter for selection model.}

\item{tree_power_y}{Tree prior parameter for outcome model.}

\item{tree_base_z}{Tree prior parameter for selection model.}

\item{tree_base_y}{Tree prior parameter for outcome model.}

\item{node.prior}{(dbarts option) An expression of the form dbarts:::normal or dbarts:::normal(k) that sets the prior used on the averages within nodes.}

\item{resid.prior}{(dbarts option) An expression of the form dbarts:::chisq or dbarts:::chisq(df,quant) that sets the prior used on the residual/error variance}

\item{proposal.probs}{(dbarts option) Named numeric vector or NULL, optionally specifying the proposal rules and their probabilities. Elements should be "birth_death", "change", and "swap" to control tree change proposals, and "birth" to give the relative frequency of birth/death in the "birth_death" step.}

\item{sigmadbarts}{(dbarts option) A positive numeric estimate of the residual standard deviation. If NA, a linear model is used with all of the predictors to obtain one.}

\item{print.opt}{Print every print.opt number of Gibbs samples.}

\item{accelerate}{If TRUE, add extra parameter for accelerated sampler as descibed by Omori (2007).}

\item{cov_prior}{Prior for the covariance of the error terms. If VH, apply the prior of van Hasselt (2011), N(gamma0, tau*phi), imposing dependence between gamma and phi. If Omori, apply the prior N(gamma0,G0). If mixture, then a mixture of the VH and Omori priors with probability mixprob applied to the VH prior.}

\item{tau}{Parameter for the prior of van Hasselt (2011) on the covariance of the error terms.}

\item{M_mat}{Base distribution covariace for errors in outcome and selection equation for Dirichlet Process mixture.}

\item{alpha_prior}{The prior for the alpha parameter of the Dirichlet Process mixture of normals. If "vh" then apply the Gamma(c1,c2) prior of van Hasselt (2011) and Escobar (1994). If "george", then apply the prior of George (2019), McCulloch (2021), Conley (2008), and Antoniak (1974).}

\item{c1}{If alpha_prior == "vh", then c1 is the shape parameter of the Gamma distribution.}

\item{c2}{If alpha_prior == "vh", then c2 is the rate parameter of the Gamma distribution.}

\item{alpha_gridsize}{If alpha_prior = "george", this is the size of the grid to use for the discretized samples of alpha}

\item{mixstep}{If equal to TRUE, includes mixing step for samplinod Dirichlet Process micture parameters.}

\item{sparse}{If equal to TRUE, use Linero Dirichlet prior on splitting probabilities}

\item{alpha_a_y}{Linero alpha prior parameter for outcome equation splitting probabilities}

\item{alpha_b_y}{Linero alpha prior parameter for outcome equation splitting probabilities}

\item{alpha_a_z}{Linero alpha prior parameter for selection equation splitting probabilities}

\item{alpha_b_z}{Linero alpha prior parameter for selection equation splitting probabilities}

\item{alpha_split_prior}{If true, set hyperprior for Linero alpha parameter}
}
\value{
The following objects are returned:
\item{Z.mat_train}{Matrix of draws of censoring model latent outcomes for training observations. Number of rows equals number of training observations. Number of columns equals n.iter . Rows are ordered in order of observations in the training data.}
\item{Z.mat_test}{Matrix of draws of censoring model latent outcomes for test observations. Number of rows equals number of test observations. Number of columns equals n.iter . Rows are ordered in order of observations in the test data.}
\item{Y.mat_train}{Matrix of draws of outcome model latent outcomes for training observations. Number of rows equals number of training observations. Number of columns equals n.iter . Rows are ordered in order of observations in the training data.}
\item{Y.mat_test}{Matrix of draws of outcome model latent outcomes for test observations. Number of rows equals number of test observations. Number of columns equals n.iter . Rows are ordered in order of observations in the test data.}
\item{mu_y_train}{Matrix of draws of the outcome model sums of terminal nodes, i.e. f(x_i), for all training observations. Number of rows equals number of training observations. Number of columns equals n.iter .}
\item{mu_y_test}{Matrix of draws of the outcome model sums of terminal nodes, i.e. f(x_i), for all test observations. Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{mucens_y_train}{Matrix of draws of the outcome model sums of terminal nodes, i.e. f(x_i), for all censored training observations. Number of rows equals number of censored training observations. Number of columns equals n.iter .}
\item{muuncens_y_train}{Matrix of draws of the outcome model sums of terminal nodes, i.e. f(x_i), for all uncensored training observations. Number of rows equals number of uncensored training observations. Number of columns equals n.iter .}
\item{mu_z_train}{Matrix of draws of the censoring model sums of terminal nodes, i.e. f(w_i), for all training observations. Number of rows equals number of training observations. Number of columns equals n.iter .}
\item{mu_z_test}{Matrix of draws of the censoring model sums of terminal nodes, i.e. f(w_i), for all test observations. Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{train.probcens}{Matrix of draws of probabilities of training sample observations being censored. Number of rows equals number of training observations. Number of columns equals n.iter .}
\item{test.probcens}{Matrix of draws of probabilities of test sample observations being censored. Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{cond_exp_train}{Matrix of draws of the conditional (i.e. possibly censored) expectations of the outcome for all training observations. Number of rows equals number of training observations. Number of columns equals n.iter .}
\item{cond_exp_test}{Matrix of draws of the conditional (i.e. possibly censored) expectations of the outcome for all test observations. Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{uncond_exp_train}{Only defined if censored_value is a number. Matrix of draws of the unconditional (i.e. possibly censored) expectations of the outcome for all training observations. Number of rows equals number of training observations. Number of columns equals n.iter .}
\item{uncond_exp_test}{Only defined if censored_value is a number. Matrix of draws of the unconditional (i.e. possibly censored) expectations of the outcome for all test observations. Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{ystar_train}{Matrix of training sample draws of the outcome assuming uncensored. Number of rows equals number of training observations. Number of columns equals n.iter .}
\item{ystar_test}{Matrix of test sample draws of the outcome assuming uncensored . Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{zstar_train}{Matrix of training sample draws of the censoring model latent outcome. Number of rows equals number of training observations. Number of columns equals n.iter.}
\item{zstar_test}{Matrix of test sample draws of the censoring model latent outcome. Number of rows equals number of test observations. Number of columns equals n.iter.}
\item{ydraws_train}{Only defined if censored_value is a number. Matrix of training sample unconditional (i.e. possibly censored) draws of the outcome. Number of rows equals number of training observations. Number of columns equals n.iter .}
\item{ydraws_test}{Only defined if censored_value is a number. Matrix of test sample unconditional (i.e. possibly censored) draws of the outcome . Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{ycond_draws_train}{List of training sample conditional (i.e. zstar >0 for draw) draws of the outcome. Number of rows equals number of training observations. Number of columns equals n.iter .}
\item{ycond_draws_test}{List of test sample conditional (i.e. zstar >0 for draw) draws of the outcome . Number of rows equals number of test observations. Number of columns equals n.iter .}
\item{Sigma_draws}{3 dimensional array of MCMC draws of the covariance matrix for the censoring and outcome error terms. The numbers of rows and columns equal are equal to 2. The first row and column correspond to the censoring model. The second row and column correspond to the outcome model. The number of slices equals n.iter . }
\item{alpha_s_y_store}{For Dirichlet prior on splitting probabilities in outcome equation, vector of alpha hyperparameter draws for each iteration.}
\item{alpha_s_z_store}{For Dirichlet prior on splitting probabilities in selection equation, vector of alpha hyperparameter draws for each iteration }
\item{var_count_y_store}{Matrix of counts of splits on each variable in outcome observation. The number of rows is the number of potential splitting variables. The number of columns is the number of post-burn-in iterations.}
\item{var_count_z_store}{Matrix of counts of splits on each variable in selection observation. The number of rows is the number of potential splitting variables. The number of columns is the number of post-burn-in iterations. }
\item{s_prob_y_store}{Splitting probabilities for the outcome equation. The number of rows is the number of potential splitting variables. The number of columns is the number of post-burn-in iterations. }
\item{s_prob_z_store}{Splitting probabilities for the selection equation. The number of rows is the number of potential splitting variables. The number of columns is the number of post-burn-in iterations. }
}
\description{
Nonparametric Type II Tobit Bayesian Additive Regression Trees implemented using MCMC. The errors in the selection and outcome equations are modelled by a Dirichlet Process mixture of bivariate normal distributions.
}
\examples{

#example taken from Zhang, J., Li, Z., Song, X., & Ning, H. (2021). Deep Tobit networks: A novel machine learning approach to microeconometrics. Neural Networks, 144, 279-296.



#Type II tobit simulation

num_train <- 5000

#consider increasing the number of covariates

Xmat_train <- matrix(NA,nrow = num_train,
                     ncol = 8)

Xmat_train[,1] <- runif(num_train, min = -1, max = 1)
Xmat_train[,2] <- rf(num_train,20,20)
Xmat_train[,3] <- rbinom(num_train, size = 1, prob = 0.75)
Xmat_train[,4] <- rnorm(num_train, mean = 1, sd = 1)
Xmat_train[,5] <- rnorm(num_train)
Xmat_train[,6] <- rbinom(num_train, size = 1, prob = 0.5)
Xmat_train[,7] <- rf(num_train,20,200)
Xmat_train[,8] <- runif(num_train, min = 0, max = 2)

#it would be better to test performance of the models when there is correlation in the error terms.
varepsilon1_train <- rnorm(num_train, mean = 0, sd = sqrt(0.00025))
varepsilon2_train <- rnorm(num_train, mean = 0, sd = sqrt(0.00025))

y1star_train <- 1 - 0.75*Xmat_train[,1] + 0.75*Xmat_train[,2] -
  0.5*Xmat_train[,4] -  0.5*Xmat_train[,6] - 0.25*Xmat_train[,1]^2 -
  0.75*Xmat_train[,1]*Xmat_train[,4] - 0.25*Xmat_train[,1]*Xmat_train[,2] -
  1*Xmat_train[,1]*Xmat_train[,6] + 0.5*Xmat_train[,2]*Xmat_train[,6] +
  varepsilon1_train

y2star_train <- 1 + 0.25*Xmat_train[,4] - 0.75*Xmat_train[,6] +
  0.5*Xmat_train[,7] + 0.25*Xmat_train[,8] +
  0.25*Xmat_train[,4]^2 + 0.75*Xmat_train[,7]^2 + 0.5*Xmat_train[,8]^2 -
  1*Xmat_train[,4]*Xmat_train[,6] + 0.5*Xmat_train[,4]*Xmat_train[,8] +
  1*Xmat_train[,6]*Xmat_train[,7] - 0.25*Xmat_train[,7]*Xmat_train[,8] +
  varepsilon2_train

y2obs_train <- ifelse(y1star_train>0, y2star_train,0)

#Type II tobit simulation

num_test <- 5000

#consider increasing the number of covariates

Xmat_test <- matrix(NA,nrow = num_test,
                    ncol = 8)

Xmat_test[,1] <- runif(num_test, min = -1, max = 1)
Xmat_test[,2] <- rf(num_test,20,20)
Xmat_test[,3] <- rbinom(num_test, size = 1, prob = 0.75)
Xmat_test[,4] <- rnorm(num_test, mean = 1, sd = 1)
Xmat_test[,5] <- rnorm(num_test)
Xmat_test[,6] <- rbinom(num_test, size = 1, prob = 0.5)
Xmat_test[,7] <- rf(num_test,20,200)
Xmat_test[,8] <- runif(num_test, min = 0, max = 2)

#it would be better to test performance of the models when there is correlation in the error terms.
varepsilon1_test <- rnorm(num_test, mean = 0, sd = sqrt(0.00025))
varepsilon2_test <- rnorm(num_test, mean = 0, sd = sqrt(0.00025))

y1star_test <- 1 - 0.75*Xmat_test[,1] + 0.75*Xmat_test[,2] -
  0.5*Xmat_test[,4] -  0.5*Xmat_test[,6] - 0.25*Xmat_test[,1]^2 -
  0.75*Xmat_test[,1]*Xmat_test[,4] - 0.25*Xmat_test[,1]*Xmat_test[,2] -
  1*Xmat_test[,1]*Xmat_test[,6] + 0.5*Xmat_test[,2]*Xmat_test[,6] +
  varepsilon1_test

y2star_test <- 1 + 0.25*Xmat_test[,4] - 0.75*Xmat_test[,6] +
  0.5*Xmat_test[,7] + 0.25*Xmat_test[,8] +
  0.25*Xmat_test[,4]^2 + 0.75*Xmat_test[,7]^2 + 0.5*Xmat_test[,8]^2 -
  1*Xmat_test[,4]*Xmat_test[,6] + 0.5*Xmat_test[,4]*Xmat_test[,8] +
  1*Xmat_test[,6]*Xmat_test[,7] - 0.25*Xmat_test[,7]*Xmat_test[,8] +
  varepsilon2_test

y2obs_test <- ifelse(y1star_test>0, y2star_test,0)

y2response_test <- ifelse(y1star_test>0, 1,0)

tbartII_example <- tbart2c(Xmat_train,
                           Xmat_test,
                           Xmat_train,
                           Xmat_test,
                           y2obs_train,
                           n.iter=5000,
                           n.burnin=1000,
                           censored_value = 0)


pred_probs_tbart2_test <- rowMeans(tbartII_example$test.probcens)

#Training (within-sample) Prediction Realization Table

cutoff_point <- mean(y2obs_train>0)

test_bin_preds <- ifelse(1 - pred_probs_tbart2_test > cutoff_point,1,0)

#Training (within-sample) Prediction Realization Table

pred_realization_test <- rbind(cbind(table(y2response_test, test_bin_preds)/length(y2response_test),
                                     apply(table(y2response_test, test_bin_preds)/length(y2response_test),1,sum)),
                               c(t(apply(table(y2response_test, test_bin_preds)/length(y2response_test),2,sum)), 1))

hit_rate_test <- pred_realization_test[1,1] +pred_realization_test[2,2]

testpreds_tbart2 <- rowMeans(tbartII_example$uncond_exp_test)

sqrt(mean((y2obs_test - testpreds_tbart2  )^2 ))

}
